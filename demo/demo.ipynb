{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import uuid\n",
    "from typing import Generator, List\n",
    "\n",
    "from bs4 import BeautifulSoup, Doctype, NavigableString, Tag\n",
    "\n",
    "from langchain.storage import InMemoryByteStore\n",
    "from langchain_core.documents import Document\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain_community.document_loaders import RecursiveUrlLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.utils.html import PREFIXES_TO_IGNORE_REGEX\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env OPENAI_API_KEY="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metadata_extractor(html: str, url: str) -> dict:\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    \n",
    "    title = soup.find(\"title\")\n",
    "    description = soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
    "    html = soup.find(\"html\")\n",
    "\n",
    "    title_paragraphs = soup.find_all('p', class_='StylePalatino24ptBoldCentered')\n",
    "    doc_metas = soup.find_all('p', class_='StylePalatino10ptBoldCentered')\n",
    "    \n",
    "    real_title = \"XrootD \"\n",
    "    release_date = \"\"\n",
    "    software_version = \"\"\n",
    "    author = \"\"\n",
    "    # Loop through the found elements and print their text if they are not empty\n",
    "    for p in title_paragraphs:\n",
    "        if p.text.strip() and not p.text.strip().isspace():\n",
    "           real_title += p.text.strip()\n",
    "\n",
    "    doc_meta_ct = 0\n",
    "    for p in doc_metas:\n",
    "        if p.text.strip() and not p.text.strip().isspace():\n",
    "            if doc_meta_ct == 0:\n",
    "                release_date = p.text.strip()\n",
    "            elif doc_meta_ct == 1:\n",
    "                software_version = p.text.strip()\n",
    "            elif doc_meta_ct == 2:\n",
    "                author = p.text.strip()\n",
    "            doc_meta_ct += 1\n",
    "\n",
    "    h1_tag = soup.find('h1')\n",
    "    introduction = \"\"\n",
    "    if h1_tag is not None:\n",
    "        # Find the first non-empty <p> following the <h1>\n",
    "        current_tag = h1_tag.find_next_sibling()\n",
    "\n",
    "        while current_tag:\n",
    "            if current_tag.name == 'p' and current_tag.text.strip():\n",
    "                break\n",
    "            current_tag = current_tag.find_next_sibling()\n",
    "        \n",
    "        introduction = current_tag.text.strip() if current_tag is not None else \"\"\n",
    "\n",
    "    return {\n",
    "        \"source\": url,\n",
    "        \"title\": real_title if title else title.get_text() if title else \"\",\n",
    "        \"documentation_release_date\": release_date,\n",
    "        \"xrootd_software_version\": software_version,\n",
    "        \"documentation_author\": author,\n",
    "        \"description\": introduction if introduction else description.get(\"content\", \"\") if description else \"\",\n",
    "        \"language\": html.get(\"lang\", \"en\") if html else \"en\",\n",
    "    }\n",
    "\n",
    "SUFFIXES_TO_IGNORE = (\n",
    "    \".css\",\n",
    "    \".js\",\n",
    "    \".ico\",\n",
    "    \".png\",\n",
    "    \".jpg\",\n",
    "    \".jpeg\",\n",
    "    \".gif\",\n",
    "    \".svg\",\n",
    "    \".csv\",\n",
    "    \".bz2\",\n",
    "    \".zip\",\n",
    "    \".epub\",\n",
    "    \".pdf\",\n",
    "    \".pptx\"\n",
    ")\n",
    "SUFFIXES_TO_IGNORE_REGEX = (\n",
    "    \"(?!\" + \"|\".join([re.escape(s) + r\"[\\#'\\\"]\" for s in SUFFIXES_TO_IGNORE]) + \")\"\n",
    ")\n",
    "\n",
    "def langchain_docs_extractor(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    # Remove all the tags that are not meaningful for the extraction.\n",
    "    SCAPE_TAGS = [\"nav\", \"footer\", \"aside\", \"script\", \"style\"]\n",
    "    [tag.decompose() for tag in soup.find_all(SCAPE_TAGS)]\n",
    "    \n",
    "    classes_to_remove = ['MsoToc1', 'MsoToc2', 'MsoToc3', 'MsoToc4']\n",
    "\n",
    "    # Find and decompose all <p> elements with specified classes\n",
    "    for class_name in classes_to_remove:\n",
    "        for element in soup.find_all('p', class_=class_name):\n",
    "            element.decompose()    \n",
    "\n",
    "    def get_text(tag: Tag) -> Generator[str, None, None]:\n",
    "        for child in tag.children:\n",
    "            if isinstance(child, Doctype):\n",
    "                continue\n",
    "\n",
    "            if isinstance(child, NavigableString):\n",
    "                yield child\n",
    "            elif isinstance(child, Tag):\n",
    "                if child.name in [\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"]:\n",
    "                    yield f\"{'#' * int(child.name[1:])} {child.get_text()}\\n\\n\"\n",
    "                elif child.name == \"a\":\n",
    "                    yield f\"[{child.get_text(strip=False)}]({child.get('href')})\"\n",
    "                elif child.name == \"img\":\n",
    "                    yield f\"![{child.get('alt', '')}]({child.get('src')})\"\n",
    "                elif child.name in [\"strong\", \"b\"]:\n",
    "                    yield f\"**{child.get_text(strip=False)}**\"\n",
    "                elif child.name in [\"em\", \"i\"]:\n",
    "                    yield f\"_{child.get_text(strip=False)}_\"\n",
    "                elif child.name == \"br\":\n",
    "                    yield \"\\n\"\n",
    "                elif child.name == \"code\":\n",
    "                    parent = child.find_parent()\n",
    "                    if parent is not None and parent.name == \"pre\":\n",
    "                        classes = parent.attrs.get(\"class\", \"\")\n",
    "\n",
    "                        language = next(\n",
    "                            filter(lambda x: re.match(r\"language-\\w+\", x), classes),\n",
    "                            None,\n",
    "                        )\n",
    "                        if language is None:\n",
    "                            language = \"\"\n",
    "                        else:\n",
    "                            language = language.split(\"-\")[1]\n",
    "\n",
    "                        lines: list[str] = []\n",
    "                        for span in child.find_all(\"span\", class_=\"token-line\"):\n",
    "                            line_content = \"\".join(\n",
    "                                token.get_text() for token in span.find_all(\"span\")\n",
    "                            )\n",
    "                            lines.append(line_content)\n",
    "\n",
    "                        code_content = \"\\n\".join(lines)\n",
    "                        yield f\"```{language}\\n{code_content}\\n```\\n\\n\"\n",
    "                    else:\n",
    "                        yield f\"`{child.get_text(strip=False)}`\"\n",
    "\n",
    "                elif child.name == \"p\":\n",
    "                    yield from get_text(child)\n",
    "                    yield \"\\n\\n\"\n",
    "                elif child.name == \"ul\":\n",
    "                    for li in child.find_all(\"li\", recursive=False):\n",
    "                        yield \"- \"\n",
    "                        yield from get_text(li)\n",
    "                        yield \"\\n\\n\"\n",
    "                elif child.name == \"ol\":\n",
    "                    for i, li in enumerate(child.find_all(\"li\", recursive=False)):\n",
    "                        yield f\"{i + 1}. \"\n",
    "                        yield from get_text(li)\n",
    "                        yield \"\\n\\n\"\n",
    "                elif child.name == \"div\" and \"tabs-container\" in child.attrs.get(\n",
    "                    \"class\", [\"\"]\n",
    "                ):\n",
    "                    tabs = child.find_all(\"li\", {\"role\": \"tab\"})\n",
    "                    tab_panels = child.find_all(\"div\", {\"role\": \"tabpanel\"})\n",
    "                    for tab, tab_panel in zip(tabs, tab_panels):\n",
    "                        tab_name = tab.get_text(strip=True)\n",
    "                        yield f\"{tab_name}\\n\"\n",
    "                        yield from get_text(tab_panel)\n",
    "                elif child.name == \"div\" and child.attrs.get(\"style\",\"\") == \"border:solid windowtext 1.0pt;padding:1.0pt 4.0pt 1.0pt 4.0pt\":\n",
    "                    # xrootd codeblock style\n",
    "                    code = \"\".join(c.text +\"\\n\"  for c in child.contents)\n",
    "                    yield f\"```\\n{code}\\n```\"\n",
    "                    yield \"\\n\\n\"\n",
    "                    pass\n",
    "                elif child.name == \"table\":\n",
    "                    yield \"[table]\"\n",
    "                    thead = child.find(\"thead\")\n",
    "                    header_exists = isinstance(thead, Tag)\n",
    "                    if header_exists:\n",
    "                        headers = thead.find_all(\"th\")\n",
    "                        if headers:\n",
    "                            yield \"| \"\n",
    "                            yield \" | \".join(header.get_text() for header in headers)\n",
    "                            yield \" |\\n\"\n",
    "                            yield \"| \"\n",
    "                            yield \" | \".join(\"----\" for _ in headers)\n",
    "                            yield \" |\\n\"\n",
    "\n",
    "                    tbody = child.find(\"tbody\")\n",
    "                    tbody_exists = isinstance(tbody, Tag)\n",
    "                    if tbody_exists:\n",
    "                        for row in tbody.find_all(\"tr\"):\n",
    "                            yield \"| \"\n",
    "                            yield \" | \".join(\n",
    "                                cell.get_text(strip=True) for cell in row.find_all(\"td\")\n",
    "                            )\n",
    "                            yield \" |\\n\"\n",
    "                    else:\n",
    "                        first_row = child.find(\"tr\")\n",
    "                        headers = first_row.find_all(\"td\")\n",
    "                        yield \"| \"\n",
    "                        yield \" | \".join(header.get_text(strip=True) for header in headers)\n",
    "                        yield \" |\\n\"\n",
    "                        yield \"| \"\n",
    "                        yield \" | \".join(\"----\" for _ in headers)\n",
    "                        yield \" |\\n\"\n",
    "                        data_rows = first_row.find_next_siblings(\"tr\")                        \n",
    "                        for row in data_rows:\n",
    "                            yield \"| \"\n",
    "                            yield \" | \".join(\n",
    "                                cell.get_text(strip=True) for cell in row.find_all(\"td\")\n",
    "                            )\n",
    "                            yield \" |\\n\"\n",
    "                    yield \"\\n\\n[table]\"\n",
    "                elif child.name in [\"button\"]:\n",
    "                    continue\n",
    "                else:\n",
    "                    yield from get_text(child)\n",
    "\n",
    "    joined = \"\".join(get_text(soup)).replace('\\xa0', '').replace('\\r\\n', ' ').replace('****', '')\n",
    "    return re.sub(r\"\\n\\n+\", \"\\n\\n\", joined).strip()\n",
    "\n",
    "def simple_extractor(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    # List of classes to remove\n",
    "    classes_to_remove = ['MsoToc1', 'MsoToc2', 'MsoToc3', 'MsoToc4']\n",
    "\n",
    "    # Find and decompose all <p> elements with specified classes\n",
    "    for class_name in classes_to_remove:\n",
    "        for element in soup.find_all('p', class_=class_name):\n",
    "            element.decompose()     \n",
    "   \n",
    "    return re.sub(r'[\\s]*\\n+[\\s]*', '\\n', soup.text).strip().replace('\\xa0', '')\n",
    "\n",
    "# Load, chunk and index the contents of the blog.\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=\"https://xrootd.slac.stanford.edu/docs.html\",\n",
    "    # url=\"https://xrootd.slac.stanford.edu/doc/dev55/xrd_config.htm\",\n",
    "    max_depth=2,\n",
    "    metadata_extractor=metadata_extractor,\n",
    "    prevent_outside=True,\n",
    "    use_async=False,\n",
    "    timeout=600,\n",
    "    base_url=\"https://xrootd.slac.stanford.edu/\",\n",
    "    # Drop trailing / to avoid duplicate pages.\n",
    "    link_regex=(\n",
    "        f\"href=[\\\"']{PREFIXES_TO_IGNORE_REGEX}((?:{SUFFIXES_TO_IGNORE_REGEX}.)*?)\"\n",
    "        r\"(?:[\\#'\\\"]|\\/[\\#'\\\"])\"\n",
    "    ),\n",
    "    extractor=langchain_docs_extractor,\n",
    "    check_response_status=True,\n",
    ")\n",
    "docs = loader.load()\n",
    "docs\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "    (\"####\", \"Header 4\"),\n",
    "]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=False)\n",
    "md_header_splits: List[Document] = list()\n",
    "for doc in docs:\n",
    "    splitted = markdown_splitter.split_text(doc.page_content)    \n",
    "    for idx, split in enumerate(splitted):\n",
    "        splitted[idx].metadata = doc.metadata.copy() | split.metadata\n",
    "    md_header_splits.extend(splitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_splits: List[Document] = []\n",
    "for md_doc in md_header_splits:\n",
    "  splitted = re.split(re.escape(\"[table]\"), md_doc.page_content)\n",
    "  for split in splitted:\n",
    "    new_doc = Document(page_content=split, metadata=md_doc.metadata.copy())\n",
    "    table_splits.append(new_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_splits: List[Document] = []\n",
    "for table_doc in table_splits:\n",
    "    # Define the pattern to capture the ``` separator\n",
    "    pattern = re.compile(r'(```.*?```)|(.+?)(?=`{3}|\\Z)', re.DOTALL)\n",
    "    \n",
    "    # Find all matches of the pattern in the text\n",
    "    matches = pattern.findall(table_doc.page_content)\n",
    "    \n",
    "    # Combine the matches to form the final result\n",
    "    result = [\"\".join(match) for match in matches]\n",
    "    \n",
    "    for split in result:\n",
    "      new_doc = Document(page_content=split, metadata=table_doc.metadata.copy())\n",
    "      code_splits.append(new_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_overlap=400)\n",
    "final_splits = text_splitter.split_documents(table_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to summarize each document and store them as a side vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"doc\": lambda x: x.page_content}\n",
    "    | ChatPromptTemplate.from_template(\"Summarize the following document:\\n\\n{doc}\")\n",
    "    | ChatOpenAI(max_retries=0)\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = chain.batch(final_splits, {\"max_concurrency\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(collection_name=\"summaries\", embedding_function=OpenAIEmbeddings())\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryByteStore()\n",
    "id_key = \"doc_id\"\n",
    "# The retriever (empty to start)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    byte_store=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "doc_ids = [str(uuid.uuid4()) for _ in final_splits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_docs = [\n",
    "    Document(page_content=s, metadata=final_splits[i].metadata.copy()|{id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(summaries)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.vectorstore.add_documents(summary_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add original chunks to the vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(final_splits):\n",
    "    doc.metadata[id_key] = doc_ids[i]\n",
    "retriever.vectorstore.add_documents(final_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved = vectorstore.max_marginal_relevance_search(\"How can I configure xrootd?\")\n",
    "retrieved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the regular embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(documents=final_splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.save_local(\"../embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How can I configure xrootd?\"\n",
    "retrieved = vectorstore.max_marginal_relevance_search(query,k=3, fetch_k=3)\n",
    "retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\")\n",
    "# prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "template = \"\"\"You are an assistant for question-answering tasks.\n",
    "You will be asked questions about XRootD, or eXtended Request Daemon.\n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "Use your existing knowledge to answer the question if the retrieved context\n",
    "does not contain useful information.\n",
    "Do not mention the context to the user. Do now let user know you are given a retrieved context.\n",
    "If you don't know the answer or nothing is provided in the context, just say: Sorry, I don't know.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain.invoke(\"What is oss in xrootd?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demovenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
